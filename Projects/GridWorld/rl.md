### 1. 学习率 (α)

* **影响** ：学习率决定了新获得的信息与旧信息的权重。较高的学习率使得 Q 值快速更新，更加依赖新经验；而较低的学习率使得更新缓慢，保留了更多的历史信息。
* **决策影响** ：如果学习率过高，可能导致 Q 值的不稳定，导致学习过程波动较大；如果过低，可能导致学习速度缓慢，甚至陷入局部最优。

### 2. 折扣因子 (γ)

* **影响** ：折扣因子决定了未来奖励的重要性。取值范围在 0 到 1 之间，γ 越接近 1，未来的奖励越重要；反之，较小的 γ 更关注当前奖励。
* **决策影响** ：高折扣因子可能导致算法更长远的考虑，从而在长期策略上更为有效；而低折扣因子可能导致策略更短视，依赖即时奖励。

### 3. 奖惩值 (R)

* **影响** ：奖惩值直接影响 Q 值的更新。正奖励会增加相关状态-动作对的 Q 值，而负奖励会减少其 Q 值。
* **决策影响** ：奖励设计对算法的学习效果至关重要。合理的奖励设计可以引导代理学习到有效的策略，而不合理的奖励可能导致学习困难或误导。

> 注意： 个人心得——如果训练次数足够多，理论上可以通过以下方式减轻学习率、折扣因子和奖惩值对决策的影响。在我们算力有限的情况下，我们要权衡好（学习率、折扣因子和奖惩值）与训练次数的配比，比如说我们可以提出关于（学习率、折扣因子和奖惩值）与训练次数的公式：
>
> $ \alpha(T) = \frac{\alpha_0}{1 + \beta T} $
>
> $ \gamma(T) = \gamma_0 + \delta \cdot f(T) $
>
> $ R(T) = R_0 + \lambda \cdot g(T) $ 

其中，**α**0 是初始学习率，β 是衰减速率，T是训练次数。

其中，*γ*是初始折扣因子，**δ** 是调整幅度

其中，**R**0 是初始奖励，**λ** 是调整系数，**g**(**T**) 是根据训练进度和学习效果进行调整的函数。
